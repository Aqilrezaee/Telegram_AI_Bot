import os
import json
import asyncio
import random
import re
from rewrite_tools import rewrite_ai_response
import requests
from dotenv import load_dotenv
import google.generativeai as genai
from telegram import Update, InlineKeyboardButton, InlineKeyboardMarkup, ReplyKeyboardMarkup
from telegram.ext import (
    ApplicationBuilder,
    ContextTypes,
    MessageHandler,
    CommandHandler,
    CallbackQueryHandler,
    filters,
)

# âœ… Ø¯Ú©Ù…Ù‡â€ŒÛŒ Ø¨Ø±Ú¯Ø´Øª Ø¨Ù‡ Ù…Ù†ÙˆÛŒ Ø§Ù†ØªØ®Ø§Ø¨ Ø­Ø§Ù„Øª
def get_main_menu():
    keyboard = [
        [InlineKeyboardButton("ğŸ”™ Ø¨Ø§Ø²Ú¯Ø´Øª Ø¨Ù‡ Ù…Ù†ÙˆÛŒ Ø§Ù†ØªØ®Ø§Ø¨ Ø­Ø§Ù„Øª", callback_data="main_menu")]
    ]
    return InlineKeyboardMarkup(keyboard)

# ğŸ“¦ Ø¨Ø§Ø±Ú¯ÛŒØ±ÛŒ Ù…ØªØºÛŒØ±Ù‡Ø§ÛŒ .env
load_dotenv()
GOOGLE_API_KEY = os.getenv("GOOGLE_API_KEY")
TELEGRAM_TOKEN = os.getenv("TELEGRAM_TOKEN")
OPENROUTER_API_KEY = os.getenv("OPENROUTER_API_KEY")
USER_MODE_FILE = "user_modes.json"

# ğŸ”§ ØªÙ†Ø¸ÛŒÙ… Gemini
genai.configure(api_key=GOOGLE_API_KEY)
model = genai.GenerativeModel("deepseek/deepseek-chat-v3-0324:free")

# ğŸ§  Ù¾ÛŒÚ©Ø±Ø¨Ù†Ø¯ÛŒ ØªÙˆÙ„ÛŒØ¯ Ù…Ø­ØªÙˆØ§
generation_config = genai.GenerationConfig(
    temperature=0.5,
    top_p=0.95,
    top_k=40,
    max_output_tokens=2048,
)

# ğŸ§¾ Ù…Ø¯ÛŒØ±ÛŒØª Ø­Ø§Ù„Øª Ú©Ø§Ø±Ø¨Ø±
def load_user_mode(user_id):
    try:
        with open(USER_MODE_FILE, "r") as f:
            data = json.load(f)
            return data.get(str(user_id), "gemini")
    except:
        return "gemini"

def save_user_mode(user_id, mode):
    try:
        if os.path.exists(USER_MODE_FILE):
            with open(USER_MODE_FILE, "r") as f:
                data = json.load(f)
        else:
            data = {}
        data[str(user_id)] = mode
        with open(USER_MODE_FILE, "w") as f:
            json.dump(data, f)
    except Exception as e:
        print(f"âŒ Ø®Ø·Ø§ Ø¯Ø± Ø°Ø®ÛŒØ±Ù‡ Ø­Ø§Ù„Øª Ú©Ø§Ø±Ø¨Ø±: {e}")

# ğŸ“¡ ØªÙ…Ø§Ø³ Ø¨Ø§ OpenRouter
def ask_openrouter(prompt):
    headers = {
        "Authorization": f"Bearer {OPENROUTER_API_KEY}",
        "Content-Type": "application/json"
    }

    system_prompt = """
    ÙˆØ¸ÛŒÙÙ‡â€ŒÛŒ ØªÙˆ Ù¾Ø§Ø³Ø® Ø¯Ø§Ø¯Ù† Ø¨Ù‡ Ø³ÙˆØ§Ù„Ø§Øª Ú©Ø§Ø±Ø¨Ø± Ø¨Ù‡ Ø´Ú©Ù„ Ù…Ø³ØªÙ‚ÛŒÙ…ØŒ Ø³Ø±ÛŒØ¹ Ùˆ Ø¯Ù‚ÛŒÙ‚ Ø§Ø³Øª.
    Ù‡ÛŒÚ† Ù…Ù‚Ø¯Ù…Ù‡ØŒ ØªÙˆØ¶ÛŒØ­ Ø§Ø¶Ø§ÙÛŒØŒ ÛŒØ§ Ø¬Ù…Ø¹â€ŒØ¨Ù†Ø¯ÛŒ Ù†Ù†ÙˆÛŒØ³.
    ÙÙ‚Ø· Ø§ØµÙ„ Ø¬ÙˆØ§Ø¨ Ø±Ø§ Ø¨Ø¯Ù‡. Ø§Ø² Ø²ÛŒØ§Ø¯Ù‡â€ŒÚ¯ÙˆÛŒÛŒ Ùˆ ØªÙˆØ¶ÛŒØ­ ÙˆØ§Ø¶Ø­Ø§Øª Ø®ÙˆØ¯Ø¯Ø§Ø±ÛŒ Ú©Ù†.
    """

    data = {
        "model": "deepseek/deepseek-chat-v3-0324:free",
        "messages": [
            {"role": "system", "content": system_prompt.strip()},
            {"role": "user", "content": prompt}
        ]
    }

    try:
        res = requests.post("https://openrouter.ai/api/v1/chat/completions", headers=headers, json=data)
        res.raise_for_status()
        res_json = res.json()
        if isinstance(res_json, dict) and "choices" in res_json and len(res_json["choices"]) > 0:
            message = res_json["choices"][0].get("message", {})
            return message.get("content", "âŒ Ù¾Ø§Ø³Ø® Ù…Ø¹ØªØ¨Ø±ÛŒ Ø¯Ø±ÛŒØ§ÙØª Ù†Ø´Ø¯.")
        else:
            return f"âŒ Ù¾Ø§Ø³Ø® Ù†Ø§Ù…Ø¹ØªØ¨Ø± Ø§Ø² OpenRouter:\n{res_json}"
    except requests.exceptions.RequestException as e:
        return f"âŒ Ø®Ø·Ø§ Ø¯Ø± Ø§Ø±ØªØ¨Ø§Ø· Ø¨Ø§ OpenRouter: {e}"

# ğŸ“¡ Ø¨Ø±Ø±Ø³ÛŒ Ùˆ Ø¨Ø§Ø²Ù†ÙˆÛŒØ³ÛŒ Ø¨Ø§ OpenRouter
def check_and_rewrite_openrouter(text, user_input):
    headers = {
        "Authorization": f"Bearer {OPENROUTER_API_KEY}",
        "Content-Type": "application/json"
    }

    check_prompt = f"""
     Ú©Ø§Ø±Ø¨Ø±: Â«{user_input}Â»
    Ù…ØªÙ† ØªÙˆÙ„ÛŒØ¯Ø´Ø¯Ù‡: Â«{text}Â»

    Ø¨Ø±Ø±Ø³ÛŒ Ú©Ù† Ú©Ù‡ Ø¢ÛŒØ§ Ø§ÛŒÙ† Ù…ØªÙ† Ø¨Ù‡ Ø§Ù†Ø¯Ø§Ø²Ù‡ Ú©Ø§ÙÛŒ Ù…Ø­Ø§ÙˆØ±Ù‡â€ŒØ§ÛŒØŒ ØµÙ…ÛŒÙ…ÛŒ Ùˆ Ù…Ø«Ù„ Ú¯ÙØªâ€ŒÙˆÚ¯ÙˆÛŒ Ø±ÙˆØ²Ù…Ø±Ù‡ Ù‡Ø³Øª ÛŒØ§ Ù†Ù‡.
    Ø§Ú¯Ø± Ù…ØªÙ† Ø®ÛŒÙ„ÛŒ Ø±Ø³Ù…ÛŒØŒ Ø®Ø´Ú© ÛŒØ§ ØºÛŒØ±Ù…Ø­Ø§ÙˆØ±Ù‡â€ŒØ§ÛŒÙ‡ØŒ Ø¨Ø§Ø²Ù†ÙˆÛŒØ³ÛŒØ´ Ú©Ù† ØªØ§:
    - Ù…Ø«Ù„ ÛŒÙ‡ Ú†Øª Ø¯ÙˆØ³ØªØ§Ù†Ù‡ Ùˆ Ø·Ø¨ÛŒØ¹ÛŒ Ø¨Ø§Ø´Ù‡.
    - Ø§Ø² Ø¹Ø¨Ø§Ø±Ø§Øª Ù…Ø­Ø§ÙˆØ±Ù‡â€ŒØ§ÛŒØŒ Ø³Ø§Ø¯Ù‡ Ùˆ Ø±ÙˆØ²Ù…Ø±Ù‡ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ú©Ù† (Ù…Ø«Ù„ Ú†ÛŒØ²ÛŒ Ú©Ù‡ ØªÙˆ Ù…Ú©Ø§Ù„Ù…Ù‡ Ø¨Ø§ Ø±ÙÛŒÙ‚ Ù…ÛŒâ€ŒÚ¯ÛŒ).
    - Ø§Ø² Ú©Ù„Ù…Ø§Øª Ø³Ù†Ú¯ÛŒÙ† ÛŒØ§ Ø§Ø¯Ø¨ÛŒ Ù¾Ø±Ù‡ÛŒØ² Ú©Ù†.
    Ø§Ú¯Ø± Ù…ØªÙ† Ø¨Ù‡ Ø§Ù†Ø¯Ø§Ø²Ù‡ Ú©Ø§ÙÛŒ Ù…Ø­Ø§ÙˆØ±Ù‡â€ŒØ§ÛŒ Ùˆ Ø®ÙˆØ¨Ù‡ØŒ Ù‡Ù…ÙˆÙ† Ø±Ùˆ Ø¨Ø±Ú¯Ø±Ø¯ÙˆÙ†.
    ÙÙ‚Ø· Ù…ØªÙ† Ù†Ù‡Ø§ÛŒÛŒ (Ø¨Ø§Ø²Ù†ÙˆÛŒØ³ÛŒâ€ŒØ´Ø¯Ù‡ ÛŒØ§ Ø§ØµÙ„ÛŒ) Ø±Ùˆ Ø¨Ù†ÙˆÛŒØ³.
    """

    data = {
        "model": "google/gemini-2.0-flash-thinking-exp-1219:free",
        "messages": [
            {"role": "user", "content": check_prompt}
        ]
    }

    try:
        res = requests.post("https://openrouter.ai/api/v1/chat/completions", headers=headers, json=data)
        res.raise_for_status()
        res_json = res.json()
        if isinstance(res_json, dict) and "choices" in res_json and len(res_json["choices"]) > 0:
            return res_json["choices"][0]["message"]["content"].strip()
        return text
    except requests.exceptions.RequestException as e:
        print(f"âŒ Ø®Ø·Ø§ Ø¯Ø± Ø¨Ø±Ø±Ø³ÛŒ Ùˆ Ø¨Ø§Ø²Ù†ÙˆÛŒØ³ÛŒ OpenRouter: {e}")
        return text

# ğŸ“¡ ØªÙ…Ø§Ø³ Ø¨Ø§ DeepSeek
def ask_deepseek(prompt):
    headers = {
        "Authorization": f"Bearer {OPENROUTER_API_KEY}",
        "Content-Type": "application/json"
    }

    raw_prompt = f"""
    Ú©Ø§Ø±Ø¨Ø±: {prompt}

    Ù„Ø·ÙØ§Ù‹ Ø³Ø±ÛŒØ¹ Ùˆ Ø¯Ù‚ÛŒÙ‚ Ø¨Ù‡ Ø§ÛŒÙ† Ø³ÙˆØ§Ù„ Ù¾Ø§Ø³Ø® Ø¨Ø¯Ù‡.
    Ø§Ø² Ø§Ø¶Ø§ÙÙ‡â€ŒÚ¯ÙˆÛŒÛŒ Ùˆ Ù…Ù‚Ø¯Ù…Ù‡â€ŒÚ†ÛŒÙ†ÛŒ Ù¾Ø±Ù‡ÛŒØ² Ú©Ù†. Ø§ØµÙ„ Ù…Ø·Ù„Ø¨ Ø±Ùˆ Ø¨Ú¯Ùˆ.
    """
    data1 = {
        "model": "deepseek/deepseek-r1:free",
        "messages": [{"role": "user", "content": raw_prompt}]
    }

    try:
        res1 = requests.post("https://openrouter.ai/api/v1/chat/completions", headers=headers, json=data1)
        res1.raise_for_status()
        res_json1 = res1.json()
        if "choices" not in res_json1 or not res_json1["choices"]:
            return "âŒ Ø®Ø·Ø§ Ø¯Ø± Ø¯Ø±ÛŒØ§ÙØª Ù¾Ø§Ø³Ø® Ù…Ø±Ø­Ù„Ù‡ Ø§ÙˆÙ„ Ø§Ø² DeepSeek."
        raw_response = res_json1["choices"][0]["message"]["content"].strip()

        friendly_prompt = f"""
        Ø§ÛŒÙ† Ù¾Ø§Ø³Ø® Ø±Ùˆ Ø¨Ù‡ Ø²Ø¨ÙˆÙ†ÛŒ Ø®ÙˆØ¯Ù…ÙˆÙ†ÛŒØŒ ØµÙ…ÛŒÙ…ÛŒ Ùˆ Ø§Ù†Ø³Ø§Ù†ÛŒ Ø¨Ø§Ø²Ù†ÙˆÛŒØ³ÛŒ Ú©Ù†. Ù†Ù‡ Ø®ÛŒÙ„ÛŒ Ø±Ø³Ù…ÛŒ Ø¨Ø§Ø´Ù‡ØŒ Ù†Ù‡ Ù¾ÛŒÚ†ÛŒØ¯Ù‡.
        Ù¾Ø§Ø³Ø® Ø§ØµÙ„ÛŒ:
        Â«{raw_response}Â»

        Ø­Ø§Ù„Ø§ Ø¬ÙˆØ§Ø¨ Ø®ÙˆØ¯Ù…ÙˆÙ†ÛŒ Ø±Ùˆ Ø¨Ù†ÙˆÛŒØ³:
        """
        data2 = {
            "model": "deepseek/deepseek-chat-v3-0324:free",
            "messages": [{"role": "user", "content": friendly_prompt}]
        }

        res2 = requests.post("https://openrouter.ai/api/v1/chat/completions", headers=headers, json=data2)
        res2.raise_for_status()
        res_json2 = res2.json()
        if "choices" not in res_json2 or not res_json2["choices"]:
            return raw_response

        friendly_response = res_json2["choices"][0]["message"]["content"].strip()
        return friendly_response

    except requests.exceptions.RequestException as e:
        return f"âŒ Ø®Ø·Ø§ Ø¯Ø± Ø§Ø±ØªØ¨Ø§Ø· Ø¨Ø§ DeepSeek: {e}"

# ğŸ“¡ Ø¨Ø±Ø±Ø³ÛŒ Ùˆ Ø¨Ø§Ø²Ù†ÙˆÛŒØ³ÛŒ Ø¨Ø§ DeepSeek
def check_and_rewrite_deepseek(text, user_input):
    headers = {
        "Authorization": f"Bearer {OPENROUTER_API_KEY}",
        "Content-Type": "application/json"
    }

    check_prompt = f"""
     Ú©Ø§Ø±Ø¨Ø±: Â«{user_input}Â»
    Ù…ØªÙ† ØªÙˆÙ„ÛŒØ¯Ø´Ø¯Ù‡: Â«{text}Â»

    Ø¨Ø±Ø±Ø³ÛŒ Ú©Ù† Ú©Ù‡ Ø¢ÛŒØ§ Ø§ÛŒÙ† Ù…ØªÙ† Ø¨Ù‡ Ø§Ù†Ø¯Ø§Ø²Ù‡ Ú©Ø§ÙÛŒ Ù…Ø­Ø§ÙˆØ±Ù‡â€ŒØ§ÛŒØŒ ØµÙ…ÛŒÙ…ÛŒ Ùˆ Ù…Ø«Ù„ Ú¯ÙØªâ€ŒÙˆÚ¯ÙˆÛŒ Ø±ÙˆØ²Ù…Ø±Ù‡ Ù‡Ø³Øª ÛŒØ§ Ù†Ù‡.
    Ø§Ú¯Ø± Ù…ØªÙ† Ø®ÛŒÙ„ÛŒ Ø±Ø³Ù…ÛŒØŒ Ø®Ø´Ú© ÛŒØ§ ØºÛŒØ±Ù…Ø­Ø§ÙˆØ±Ù‡â€ŒØ§ÛŒÙ‡ØŒ Ø¨Ø§Ø²Ù†ÙˆÛŒØ³ÛŒØ´ Ú©Ù† ØªØ§:
    - Ù…Ø«Ù„ ÛŒÙ‡ Ú†Øª Ø¯ÙˆØ³ØªØ§Ù†Ù‡ Ùˆ Ø·Ø¨ÛŒØ¹ÛŒ Ø¨Ø§Ø´Ù‡.
    - Ø§Ø² Ø¹Ø¨Ø§Ø±Ø§Øª Ù…Ø­Ø§ÙˆØ±Ù‡â€ŒØ§ÛŒØŒ Ø³Ø§Ø¯Ù‡ Ùˆ Ø±ÙˆØ²Ù…Ø±Ù‡ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ú©Ù† (Ù…Ø«Ù„ Ú†ÛŒØ²ÛŒ Ú©Ù‡ ØªÙˆ Ù…Ú©Ø§Ù„Ù…Ù‡ Ø¨Ø§ Ø±ÙÛŒÙ‚ Ù…ÛŒâ€ŒÚ¯ÛŒ).
    - Ø§Ø² Ú©Ù„Ù…Ø§Øª Ø³Ù†Ú¯ÛŒÙ† ÛŒØ§ Ø§Ø¯Ø¨ÛŒ Ù¾Ø±Ù‡ÛŒØ² Ú©Ù†.
    Ø§Ú¯Ø± Ù…ØªÙ† Ø¨Ù‡ Ø§Ù†Ø¯Ø§Ø²Ù‡ Ú©Ø§ÙÛŒ Ù…Ø­Ø§ÙˆØ±Ù‡â€ŒØ§ÛŒ Ùˆ Ø®ÙˆØ¨Ù‡ØŒ Ù‡Ù…ÙˆÙ† Ø±Ùˆ Ø¨Ø±Ú¯Ø±Ø¯ÙˆÙ†.
    ÙÙ‚Ø· Ù…ØªÙ† Ù†Ù‡Ø§ÛŒÛŒ (Ø¨Ø§Ø²Ù†ÙˆÛŒØ³ÛŒâ€ŒØ´Ø¯Ù‡ ÛŒØ§ Ø§ØµÙ„ÛŒ) Ø±Ùˆ Ø¨Ù†ÙˆÛŒØ³.
    """

    data = {
        "model": "deepseek/deepseek-chat-v3-0324:free",
        "messages": [
            {"role": "user", "content": check_prompt}
        ]
    }

    try:
        res = requests.post("https://openrouter.ai/api/v1/chat/completions", headers=headers, json=data)
        res.raise_for_status()
        res_json = res.json()
        if isinstance(res_json, dict) and "choices" in res_json and len(res_json["choices"]) > 0:
            return res_json["choices"][0]["message"]["content"].strip()
        return text
    except requests.exceptions.RequestException as e:
        print(f"âŒ Ø®Ø·Ø§ Ø¯Ø± Ø¨Ø±Ø±Ø³ÛŒ Ùˆ Ø¨Ø§Ø²Ù†ÙˆÛŒØ³ÛŒ DeepSeek: {e}")
        return text

# ğŸ§  Ù¾Ø±Ø¯Ø§Ø²Ø´ Ù¾ÛŒØ§Ù… Ù…ØªÙ†ÛŒ
async def process_message(user_input, mode="gemini"):
    try:
        print(f"ğŸ› ï¸ Ø´Ø±ÙˆØ¹ Ø¨Ø§: {mode}")

        if mode == "gemini":
            prompt = f"""
             Ú©Ø§Ø±Ø¨Ø±:
            Â«{user_input}Â»

            Ù„Ø·ÙØ§Ù‹ Ø¨Ù‡ ØµÙˆØ±Øª ÙˆØ§Ø¶Ø­ØŒ Ø¯Ù‚ÛŒÙ‚ØŒ Ùˆ Ù‚Ø§Ø¨Ù„ ÙÙ‡Ù… Ø¨Ø±Ø§ÛŒ Ø§Ù†Ø³Ø§Ù† Ø¨Ù‡ Ø§ÛŒÙ† Ø³ÙˆØ§Ù„ Ù¾Ø§Ø³Ø® Ø¨Ø¯Ù‡.
            Ø§Ø² Ù„Ø­Ù† ØµÙ…ÛŒÙ…ÛŒ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ú©Ù† Ùˆ Ù¾Ø§Ø³Ø® Ú©Ø§Ø±Ø¨Ø±Ø¯ÛŒ Ø¨Ø¯Ù‡.
            """
            print("ğŸ“ Ù¾ÛŒØ§Ù… Ø³Ø§Ø®ØªÙ‡ Ø´Ø¯\n", prompt)

            try:
                response_raw = model.generate_content(prompt, generation_config=generation_config)
                print("âœ… Ø®Ø±ÙˆØ¬ÛŒ Ø®Ø§Ù… Gemini:", response_raw)
                response = response_raw.text.strip() if response_raw and response_raw.text else ""
                print("ğŸ“¤ Ù¾Ø§Ø³Ø® Ø§ÙˆÙ„ÛŒÙ‡ Gemini:", response)
            except Exception as e:
                print("âŒ Ø®Ø·Ø§ Ø¯Ø± ÙØ±Ø§Ø®ÙˆØ§Ù†ÛŒ Gemini:", e)
                return "âŒ Ø®Ø·Ø§ Ø¯Ø± Ø¯Ø±ÛŒØ§ÙØª Ù¾Ø§Ø³Ø® Ø§Ø² Gemini."

            if not response:
                return "âŒ Gemini Ù¾Ø§Ø³Ø®ÛŒ ØªÙˆÙ„ÛŒØ¯ Ù†Ú©Ø±Ø¯."

            try:
                humanized_response = rewrite_ai_response(response)
                print("ğŸŒ€ Ø®Ø±ÙˆØ¬ÛŒ Ø¨Ø§Ø²Ù†ÙˆÛŒØ³ÛŒâ€ŒØ´Ø¯Ù‡ Ø§ÙˆÙ„ÛŒÙ‡:", humanized_response)
            except Exception as e:
                print("âŒ Ø®Ø·Ø§ Ø¯Ø± Ø¨Ø§Ø²Ù†ÙˆÛŒØ³ÛŒ Ø§ÙˆÙ„ÛŒÙ‡:", e)
                return "âŒ Ù…Ø´Ú©Ù„ÛŒ Ø¯Ø± Ø¨Ø§Ø²Ù†ÙˆÛŒØ³ÛŒ Ù¾Ø§Ø³Ø® Ù¾ÛŒØ´ Ø¢Ù…Ø¯."

            # Ø¨Ø±Ø±Ø³ÛŒ Ùˆ Ø¨Ø§Ø²Ù†ÙˆÛŒØ³ÛŒ Ø¨Ø§ Gemini
            check_prompt = f"""
             Ú©Ø§Ø±Ø¨Ø±: Â«{user_input}Â»
            Ù…ØªÙ† ØªÙˆÙ„ÛŒØ¯Ø´Ø¯Ù‡: Â«{humanized_response}Â»

            Ø¨Ø±Ø±Ø³ÛŒ Ú©Ù† Ú©Ù‡ Ø¢ÛŒØ§ Ø§ÛŒÙ† Ù…ØªÙ† Ø¨Ù‡ Ø§Ù†Ø¯Ø§Ø²Ù‡ Ú©Ø§ÙÛŒ Ù…Ø­Ø§ÙˆØ±Ù‡â€ŒØ§ÛŒØŒ ØµÙ…ÛŒÙ…ÛŒ Ùˆ Ù…Ø«Ù„ Ú¯ÙØªâ€ŒÙˆÚ¯ÙˆÛŒ Ø±ÙˆØ²Ù…Ø±Ù‡ Ù‡Ø³Øª ÛŒØ§ Ù†Ù‡.
            Ø§Ú¯Ø± Ù…ØªÙ† Ø®ÛŒÙ„ÛŒ Ø±Ø³Ù…ÛŒØŒ Ø®Ø´Ú© ÛŒØ§ ØºÛŒØ±Ù…Ø­Ø§ÙˆØ±Ù‡â€ŒØ§ÛŒÙ‡ØŒ Ø¨Ø§Ø²Ù†ÙˆÛŒØ³ÛŒØ´ Ú©Ù† ØªØ§:
            - Ù…Ø«Ù„ ÛŒÙ‡ Ú†Øª Ø¯ÙˆØ³ØªØ§Ù†Ù‡ Ùˆ Ø·Ø¨ÛŒØ¹ÛŒ Ø¨Ø§Ø´Ù‡.
            - Ø§Ø² Ø¹Ø¨Ø§Ø±Ø§Øª Ù…Ø­Ø§ÙˆØ±Ù‡â€ŒØ§ÛŒØŒ Ø³Ø§Ø¯Ù‡ Ùˆ Ø±ÙˆØ²Ù…Ø±Ù‡ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ú©Ù† (Ù…Ø«Ù„ Ú†ÛŒØ²ÛŒ Ú©Ù‡ ØªÙˆ Ù…Ú©Ø§Ù„Ù…Ù‡ Ø¨Ø§ Ø±ÙÛŒÙ‚ Ù…ÛŒâ€ŒÚ¯ÛŒ).
            - Ø§Ø² Ú©Ù„Ù…Ø§Øª Ø³Ù†Ú¯ÛŒÙ† ÛŒØ§ Ø§Ø¯Ø¨ÛŒ Ù¾Ø±Ù‡ÛŒØ² Ú©Ù†.
            Ø§Ú¯Ø± Ù…ØªÙ† Ø¨Ù‡ Ø§Ù†Ø¯Ø§Ø²Ù‡ Ú©Ø§ÙÛŒ Ù…Ø­Ø§ÙˆØ±Ù‡â€ŒØ§ÛŒ Ùˆ Ø®ÙˆØ¨Ù‡ØŒ Ù‡Ù…ÙˆÙ† Ø±Ùˆ Ø¨Ø±Ú¯Ø±Ø¯ÙˆÙ†.
            ÙÙ‚Ø· Ù…ØªÙ† Ù†Ù‡Ø§ÛŒÛŒ (Ø¨Ø§Ø²Ù†ÙˆÛŒØ³ÛŒâ€ŒØ´Ø¯Ù‡ ÛŒØ§ Ø§ØµÙ„ÛŒ) Ø±Ùˆ Ø¨Ù†ÙˆÛŒØ³.
            """
            try:
                conversational_response = model.generate_content(check_prompt, generation_config=generation_config).text.strip()
                print("ğŸ“ Ù…ØªÙ† Ù…Ø­Ø§ÙˆØ±Ù‡â€ŒØ§ÛŒ Gemini:", conversational_response)
            except Exception as e:
                print("âŒ Ø®Ø·Ø§ Ø¯Ø± Ø¨Ø±Ø±Ø³ÛŒ Ùˆ Ø¨Ø§Ø²Ù†ÙˆÛŒØ³ÛŒ Gemini:", e)
                conversational_response = humanized_response

            # Ø§Ù†Ø³Ø§Ù†ÛŒâ€ŒØ³Ø§Ø²ÛŒ Ù†Ù‡Ø§ÛŒÛŒ
            try:
                final_response = rewrite_ai_response(conversational_response)
                print("ğŸŒ€ Ø®Ø±ÙˆØ¬ÛŒ Ø¨Ø§Ø²Ù†ÙˆÛŒØ³ÛŒâ€ŒØ´Ø¯Ù‡ Ù†Ù‡Ø§ÛŒÛŒ:", final_response)
            except Exception as e:
                print("âŒ Ø®Ø·Ø§ Ø¯Ø± Ø¨Ø§Ø²Ù†ÙˆÛŒØ³ÛŒ Ù†Ù‡Ø§ÛŒÛŒ:", e)
                final_response = conversational_response

            return final_response

        elif mode == "openrouter":
            response = await asyncio.to_thread(ask_openrouter, user_input)
            if "âŒ" in response or not response.strip():
                return response

            humanized_response = rewrite_ai_response(response)
            print("ğŸŒ€ Ø®Ø±ÙˆØ¬ÛŒ Ø¨Ø§Ø²Ù†ÙˆÛŒØ³ÛŒâ€ŒØ´Ø¯Ù‡ Ø§ÙˆÙ„ÛŒÙ‡ OpenRouter:", humanized_response)

            conversational_response = await asyncio.to_thread(check_and_rewrite_openrouter, humanized_response, user_input)
            print("ğŸ“ Ù…ØªÙ† Ù…Ø­Ø§ÙˆØ±Ù‡â€ŒØ§ÛŒ OpenRouter:", conversational_response)

            final_response = rewrite_ai_response(conversational_response)
            print("ğŸŒ€ Ø®Ø±ÙˆØ¬ÛŒ Ø¨Ø§Ø²Ù†ÙˆÛŒØ³ÛŒâ€ŒØ´Ø¯Ù‡ Ù†Ù‡Ø§ÛŒÛŒ OpenRouter:", final_response)
            return final_response

        elif mode == "deepseek":
            response = await asyncio.to_thread(ask_deepseek, user_input)
            if "âŒ" in response or not response.strip():
                return response

            humanized_response = rewrite_ai_response(response)
            print("ğŸŒ€ Ø®Ø±ÙˆØ¬ÛŒ Ø¨Ø§Ø²Ù†ÙˆÛŒØ³ÛŒâ€ŒØ´Ø¯Ù‡ Ø§ÙˆÙ„ÛŒÙ‡ DeepSeek:", humanized_response)

            conversational_response = await asyncio.to_thread(check_and_rewrite_deepseek, humanized_response, user_input)
            print("ğŸ“ Ù…ØªÙ† Ù…Ø­Ø§ÙˆØ±Ù‡â€ŒØ§ÛŒ DeepSeek:", conversational_response)

            final_response = rewrite_ai_response(conversational_response)
            print("ğŸŒ€ Ø®Ø±ÙˆØ¬ÛŒ Ø¨Ø§Ø²Ù†ÙˆÛŒØ³ÛŒâ€ŒØ´Ø¯Ù‡ Ù†Ù‡Ø§ÛŒÛŒ DeepSeek:", final_response)
            return final_response

        elif mode == "refined":
            openrouter_resp = await asyncio.to_thread(ask_openrouter, user_input)
            deepseek_resp = await asyncio.to_thread(ask_deepseek, user_input)

            print(f"ğŸ“¨ Ù¾Ø§Ø³Ø® OpenRouter: {openrouter_resp}")
            print(f"ğŸ“¨ Ù¾Ø§Ø³Ø® DeepSeek: {deepseek_resp}")

            if not openrouter_resp or "âŒ" in openrouter_resp:
                openrouter_resp = ""
            if not deepseek_resp or "âŒ" in deepseek_resp:
                deepseek_resp = ""

            if not openrouter_resp and not deepseek_resp:
                return "âŒ Ù‡ÛŒÚ† Ù¾Ø§Ø³Ø®ÛŒ Ø§Ø² Ù…Ø¯Ù„â€ŒÙ‡Ø§ Ø¯Ø±ÛŒØ§ÙØª Ù†Ø´Ø¯. Ù„Ø·ÙØ§Ù‹ Ø¨Ø¹Ø¯Ø§Ù‹ Ø¯ÙˆØ¨Ø§Ø±Ù‡ Ø§Ù…ØªØ­Ø§Ù† Ú©Ù†."

            responses_combined = ""
            if openrouter_resp:
                responses_combined += f"ğŸ§  Ù¾Ø§Ø³Ø® OpenRouter:\n{openrouter_resp}\n\n"
            if deepseek_resp:
                responses_combined += f"ğŸ¤– Ù¾Ø§Ø³Ø® DeepSeek:\n{deepseek_resp}"

            merge_prompt = f"""
             Ú©Ø§Ø±Ø¨Ø±:
            {user_input}

            {responses_combined}

            ğŸ” Ù„Ø·ÙØ§Ù‹ Ø¨Ø§ Ø¨Ø±Ø±Ø³ÛŒ Ù¾Ø§Ø³Ø®â€ŒÙ‡Ø§ÛŒ Ø¨Ø§Ù„Ø§ØŒ ÛŒÚ© Ø¬ÙˆØ§Ø¨ Ù†Ù‡Ø§ÛŒÛŒ ØªÙˆÙ„ÛŒØ¯ Ú©Ù† Ú©Ù‡:
            - Ø¯Ù‚ÛŒÙ‚ØŒ Ø³Ø§Ø¯Ù‡ Ùˆ Ú©Ø§Ø±Ø¨Ø±Ø¯ÛŒ Ø¨Ø§Ø´Ù‡ âœ…
            - Ù„Ø­Ù† ØµÙ…ÛŒÙ…ÛŒØŒ Ø§Ù†Ø³Ø§Ù†ÛŒ Ùˆ Ù‚Ø§Ø¨Ù„ Ø¯Ø±Ú© Ø¯Ø§Ø´ØªÙ‡ Ø¨Ø§Ø´Ù‡ ğŸ¤
            - Ø§Ø¶Ø§ÙÙ‡â€ŒÚ¯ÙˆÛŒÛŒ Ù†Ø¯Ø§Ø´ØªÙ‡ Ø¨Ø§Ø´Ù‡ âŒ
            - Ø¯Ø± ØµÙˆØ±Øª Ù†ÛŒØ§Ø² Ø§Ø² Ø§ÛŒÙ…ÙˆØ¬ÛŒ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ú©Ù† ğŸ¯

            ÙÙ‚Ø· Ù†ØªÛŒØ¬Ù‡ Ù†Ù‡Ø§ÛŒÛŒ Ø±Ùˆ Ø¨Ú¯ÙˆØŒ Ù†Ù‡ Ù…Ø±Ø§Ø­Ù„ Ù¾Ø±Ø¯Ø§Ø²Ø´ Ø±Ùˆ.
            """

            try:
                reply = model.generate_content(merge_prompt, generation_config=generation_config).text.strip()
                if not reply:
                    return "âŒ Ù¾Ø§Ø³Ø® Ù†Ù‡Ø§ÛŒÛŒ ØªÙˆÙ„ÛŒØ¯ Ù†Ø´Ø¯."

                humanized_response = rewrite_ai_response(reply)
                print("ğŸŒ€ Ø®Ø±ÙˆØ¬ÛŒ Ø¨Ø§Ø²Ù†ÙˆÛŒØ³ÛŒâ€ŒØ´Ø¯Ù‡ Ø§ÙˆÙ„ÛŒÙ‡ Refined:", humanized_response)

                check_prompt = f"""
                 Ú©Ø§Ø±Ø¨Ø±: Â«{user_input}Â»
                Ù…ØªÙ† ØªÙˆÙ„ÛŒØ¯Ø´Ø¯Ù‡: Â«{humanized_response}Â»

                Ø¨Ø±Ø±Ø³ÛŒ Ú©Ù† Ú©Ù‡ Ø¢ÛŒØ§ Ø§ÛŒÙ† Ù…ØªÙ† Ø¨Ù‡ Ø§Ù†Ø¯Ø§Ø²Ù‡ Ú©Ø§ÙÛŒ Ù…Ø­Ø§ÙˆØ±Ù‡â€ŒØ§ÛŒØŒ ØµÙ…ÛŒÙ…ÛŒ Ùˆ Ù…Ø«Ù„ Ú¯ÙØªâ€ŒÙˆÚ¯ÙˆÛŒ Ø±ÙˆØ²Ù…Ø±Ù‡ Ù‡Ø³Øª ÛŒØ§ Ù†Ù‡.
                Ø§Ú¯Ø± Ù…ØªÙ† Ø®ÛŒÙ„ÛŒ Ø±Ø³Ù…ÛŒØŒ Ø®Ø´Ú© ÛŒØ§ ØºÛŒØ±Ù…Ø­Ø§ÙˆØ±Ù‡â€ŒØ§ÛŒÙ‡ØŒ Ø¨Ø§Ø²Ù†ÙˆÛŒØ³ÛŒØ´ Ú©Ù† ØªØ§:
                - Ù…Ø«Ù„ ÛŒÙ‡ Ú†Øª Ø¯ÙˆØ³ØªØ§Ù†Ù‡ Ùˆ Ø·Ø¨ÛŒØ¹ÛŒ Ø¨Ø§Ø´Ù‡.
                - Ø§Ø² Ø¹Ø¨Ø§Ø±Ø§Øª Ù…Ø­Ø§ÙˆØ±Ù‡â€ŒØ§ÛŒØŒ Ø³Ø§Ø¯Ù‡ Ùˆ Ø±ÙˆØ²Ù…Ø±Ù‡ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ú©Ù† (Ù…Ø«Ù„ Ú†ÛŒØ²ÛŒ Ú©Ù‡ ØªÙˆ Ù…Ú©Ø§Ù„Ù…Ù‡ Ø¨Ø§ Ø±ÙÛŒÙ‚ Ù…ÛŒâ€ŒÚ¯ÛŒ).
                - Ø§Ø² Ú©Ù„Ù…Ø§Øª Ø³Ù†Ú¯ÛŒÙ† ÛŒØ§ Ø§Ø¯Ø¨ÛŒ Ù¾Ø±Ù‡ÛŒØ² Ú©Ù†.
                Ø§Ú¯Ø± Ù…ØªÙ† Ø¨Ù‡ Ø§Ù†Ø¯Ø§Ø²Ù‡ Ú©Ø§ÙÛŒ Ù…Ø­Ø§ÙˆØ±Ù‡â€ŒØ§ÛŒ Ùˆ Ø®ÙˆØ¨Ù‡ØŒ Ù‡Ù…ÙˆÙ† Ø±Ùˆ Ø¨Ø±Ú¯Ø±Ø¯ÙˆÙ†.
                ÙÙ‚Ø· Ù…ØªÙ† Ù†Ù‡Ø§ÛŒÛŒ (Ø¨Ø§Ø²Ù†ÙˆÛŒØ³ÛŒâ€ŒØ´Ø¯Ù‡ ÛŒØ§ Ø§ØµÙ„ÛŒ) Ø±Ùˆ Ø¨Ù†ÙˆÛŒØ³.
                """
                conversational_response = model.generate_content(check_prompt, generation_config=generation_config).text.strip()
                print("ğŸ“ Ù…ØªÙ† Ù…Ø­Ø§ÙˆØ±Ù‡â€ŒØ§ÛŒ Refined:", conversational_response)

                final_response = rewrite_ai_response(conversational_response)
                print("ğŸŒ€ Ø®Ø±ÙˆØ¬ÛŒ Ø¨Ø§Ø²Ù†ÙˆÛŒØ³ÛŒâ€ŒØ´Ø¯Ù‡ Ù†Ù‡Ø§ÛŒÛŒ Refined:", final_response)
                return final_response
            except Exception as e:
                print("âŒ Ø®Ø·Ø§ Ø§Ø² Gemini:", e)
                return "âŒ Ù…Ø´Ú©Ù„ÛŒ Ø¯Ø± ØªÙˆÙ„ÛŒØ¯ Ù¾Ø§Ø³Ø® Ù†Ù‡Ø§ÛŒÛŒ Ù¾ÛŒØ´ Ø¢Ù…Ø¯."

    except Exception as e:
        print("âŒ Ø®Ø·Ø§ Ø¯Ø± process_message:", e)
        return f"Ø®Ø·Ø§: {str(e)}"

# ğŸ§  Ù¾Ø±Ø¯Ø§Ø²Ø´ ØªØµÙˆÛŒØ±
async def process_image(image_path, caption, mode="gemini"):
    try:
        if mode == "gemini":
            # Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ ØªØµÙˆÛŒØ±
            with open(image_path, "rb") as img_file:
                image_data = genai.upload_file(img_file.name)
            
            # ØªÙ†Ø¸ÛŒÙ… Ù¾Ø±Ø§Ù…Ù¾Øª
            prompt = f"""
            ØªØµÙˆÛŒØ± Ø²ÛŒØ± Ø±Ø§ Ø¨Ø±Ø±Ø³ÛŒ Ú©Ù† Ùˆ Ø¨Ø± Ø§Ø³Ø§Ø³ Ø¯Ø±Ø®ÙˆØ§Ø³Øª Ú©Ø§Ø±Ø¨Ø± Ù¾Ø§Ø³Ø® Ø¨Ø¯Ù‡:
            Ø¯Ø±Ø®ÙˆØ§Ø³Øª: Â«{caption}Â»
            Ù¾Ø§Ø³Ø® Ø±Ø§ Ø¨Ù‡ ØµÙˆØ±Øª ØµÙ…ÛŒÙ…ÛŒØŒ Ù…Ø­Ø§ÙˆØ±Ù‡â€ŒØ§ÛŒ Ùˆ Ù‚Ø§Ø¨Ù„ ÙÙ‡Ù… Ø¨Ù†ÙˆÛŒØ³.
            """
            
            # Ø§Ø±Ø³Ø§Ù„ Ø¨Ù‡ Gemini
            response = model.generate_content(
                [image_data, prompt],
                generation_config=generation_config
            )
            response_text = response.text.strip() if response and response.text else "âŒ Ù¾Ø§Ø³Ø®ÛŒ Ø¯Ø±ÛŒØ§ÙØª Ù†Ø´Ø¯."
            
            # Ø¨Ø§Ø²Ù†ÙˆÛŒØ³ÛŒ Ù¾Ø§Ø³Ø® Ø¨Ø±Ø§ÛŒ Ù…Ø­Ø§ÙˆØ±Ù‡â€ŒØ§ÛŒ Ø´Ø¯Ù†
            humanized_response = rewrite_ai_response(response_text)
            
            # Ø¨Ø±Ø±Ø³ÛŒ Ùˆ Ø¨Ø§Ø²Ù†ÙˆÛŒØ³ÛŒ Ù†Ù‡Ø§ÛŒÛŒ
            check_prompt = f"""
             Ú©Ø§Ø±Ø¨Ø±: Â«{caption}Â»
            Ù…ØªÙ† ØªÙˆÙ„ÛŒØ¯Ø´Ø¯Ù‡: Â«{humanized_response}Â»
            Ø¨Ø±Ø±Ø³ÛŒ Ú©Ù† Ú©Ù‡ Ø¢ÛŒØ§ Ø§ÛŒÙ† Ù…ØªÙ† Ø¨Ù‡ Ø§Ù†Ø¯Ø§Ø²Ù‡ Ú©Ø§ÙÛŒ Ù…Ø­Ø§ÙˆØ±Ù‡â€ŒØ§ÛŒØŒ ØµÙ…ÛŒÙ…ÛŒ Ùˆ Ù…Ø«Ù„ Ú¯ÙØªâ€ŒÙˆÚ¯ÙˆÛŒ Ø±ÙˆØ²Ù…Ø±Ù‡ Ù‡Ø³Øª ÛŒØ§ Ù†Ù‡.
            Ø§Ú¯Ø± Ù…ØªÙ† Ø®ÛŒÙ„ÛŒ Ø±Ø³Ù…ÛŒØŒ Ø®Ø´Ú© ÛŒØ§ ØºÛŒØ±Ù…Ø­Ø§ÙˆØ±Ù‡â€ŒØ§ÛŒÙ‡ØŒ Ø¨Ø§Ø²Ù†ÙˆÛŒØ³ÛŒØ´ Ú©Ù† ØªØ§:
            - Ù…Ø«Ù„ ÛŒÙ‡ Ú†Øª Ø¯ÙˆØ³ØªØ§Ù†Ù‡ Ùˆ Ø·Ø¨ÛŒØ¹ÛŒ Ø¨Ø§Ø´Ù‡.
            - Ø§Ø² Ø¹Ø¨Ø§Ø±Ø§Øª Ù…Ø­Ø§ÙˆØ±Ù‡â€ŒØ§ÛŒØŒ Ø³Ø§Ø¯Ù‡ Ùˆ Ø±ÙˆØ²Ù…Ø±Ù‡ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ú©Ù†.
            - Ø§Ø² Ú©Ù„Ù…Ø§Øª Ø³Ù†Ú¯ÛŒÙ† ÛŒØ§ Ø§Ø¯Ø¨ÛŒ Ù¾Ø±Ù‡ÛŒØ² Ú©Ù†.
            Ø§Ú¯Ø± Ù…ØªÙ† Ø¨Ù‡ Ø§Ù†Ø¯Ø§Ø²Ù‡ Ú©Ø§ÙÛŒ Ù…Ø­Ø§ÙˆØ±Ù‡â€ŒØ§ÛŒ Ùˆ Ø®ÙˆØ¨Ù‡ØŒ Ù‡Ù…ÙˆÙ† Ø±Ùˆ Ø¨Ø±Ú¯Ø±Ø¯ÙˆÙ†.
            ÙÙ‚Ø· Ù…ØªÙ† Ù†Ù‡Ø§ÛŒÛŒ Ø±Ùˆ Ø¨Ù†ÙˆÛŒØ³.
            """
            conversational_response = model.generate_content(check_prompt, generation_config=generation_config).text.strip()
            final_response = rewrite_ai_response(conversational_response)
            return final_response
        
        else:
            return "âŒ Ù¾Ø±Ø¯Ø§Ø²Ø´ ØªØµÙˆÛŒØ± ÙÙ‚Ø· Ø¨Ø§ Ù…Ø¯Ù„ Gemini Ø§Ù…Ú©Ø§Ù†â€ŒÙ¾Ø°ÛŒØ± Ø§Ø³Øª. Ù„Ø·ÙØ§Ù‹ Ù…Ø¯Ù„ Gemini Ø±Ø§ Ø§Ù†ØªØ®Ø§Ø¨ Ú©Ù†ÛŒØ¯."
            
    except Exception as e:
        return f"âŒ Ø®Ø·Ø§ Ø¯Ø± Ù¾Ø±Ø¯Ø§Ø²Ø´ ØªØµÙˆÛŒØ±: {str(e)}"

# ğŸ›ï¸ Ú©ÛŒØ¨ÙˆØ±Ø¯ Ù¾Ø§ÛŒÛŒÙ† Ú†Øª
def get_persistent_keyboard():
    return ReplyKeyboardMarkup(
        keyboard=[
            ["ğŸ  Ù…Ù†ÙˆÛŒ Ø§ØµÙ„ÛŒ", "ğŸ§  ØªØºÛŒÛŒØ± Ù…Ø¯Ù„"]
        ],
        resize_keyboard=True,
        one_time_keyboard=False
    )

# ğŸ›ï¸ Ø§Ù†ØªØ®Ø§Ø¨ Ù…Ø¯Ù„ Ù‡ÙˆØ´ Ù…ØµÙ†ÙˆØ¹ÛŒ
async def show_mode_selection(update: Update, context: ContextTypes.DEFAULT_TYPE):
    keyboard = [
        [
            InlineKeyboardButton("ğŸŒŸ Gemini", callback_data="set_gemini"),
            InlineKeyboardButton("ğŸ§  OpenRouter", callback_data="set_openrouter")
        ],
        [
            InlineKeyboardButton("ğŸ¤– DeepSeek", callback_data="set_deepseek"),
            InlineKeyboardButton("ğŸ§ª ØªØ±Ú©ÛŒØ¨ÛŒ", callback_data="set_refined")
        ],
        [
            InlineKeyboardButton("ğŸ  Ø¨Ø§Ø²Ú¯Ø´Øª Ø¨Ù‡ Ù…Ù†Ùˆ", callback_data="main_menu")
        ]
    ]
    reply_markup = InlineKeyboardMarkup(keyboard)
    if update.message:
        await update.message.reply_text("Ù…Ø¯Ù„ Ù…ÙˆØ±Ø¯ Ù†Ø¸Ø± Ø±Ùˆ Ø§Ù†ØªØ®Ø§Ø¨ Ú©Ù†:", reply_markup=reply_markup)
    elif update.callback_query:
        await update.callback_query.edit_message_text("Ù…Ø¯Ù„ Ù…ÙˆØ±Ø¯ Ù†Ø¸Ø± Ø±Ùˆ Ø§Ù†ØªØ®Ø§Ø¨ Ú©Ù†:", reply_markup=reply_markup)

# ğŸ  Ù…Ù†ÙˆÛŒ Ø§ØµÙ„ÛŒ
async def show_main_menu(update: Update, context: ContextTypes.DEFAULT_TYPE):
    keyboard = [
        [InlineKeyboardButton("ğŸ§  Ø§Ù†ØªØ®Ø§Ø¨ Ù…Ø¯Ù„ Ù‡ÙˆØ´ Ù…ØµÙ†ÙˆØ¹ÛŒ", callback_data="change_model")],
        [InlineKeyboardButton("ğŸ“ Ù¾Ø±Ø³ÛŒØ¯Ù† Ø³ÙˆØ§Ù„ Ø¬Ø¯ÛŒØ¯", switch_inline_query_current_chat="")],
        [InlineKeyboardButton("â™»ï¸ Ø±ÛŒØ³Øª", callback_data="reset_all")]
    ]
    reply_markup = InlineKeyboardMarkup(keyboard)
    await update.message.reply_text("ğŸ‘‹ Ø®ÙˆØ´ Ø§ÙˆÙ…Ø¯ÛŒ! Ú†Ù‡ Ú©Ø§Ø±ÛŒ Ù…ÛŒâ€ŒØ®ÙˆØ§ÛŒ Ø§Ù†Ø¬Ø§Ù… Ø¨Ø¯ÛŒØŸ", reply_markup=reply_markup)

# â™»ï¸ Ø±ÛŒØ³Øª
async def reset_session(update: Update, context: ContextTypes.DEFAULT_TYPE):
    context.chat_data.clear()
    await update.message.reply_text("ğŸ”„ Ù‡Ù…Ù‡ Ú†ÛŒØ² Ø±ÛŒØ³Øª Ø´Ø¯. Ø§Ø² /start ÛŒØ§ /menu Ø¯ÙˆØ¨Ø§Ø±Ù‡ Ø´Ø±ÙˆØ¹ Ú©Ù†.", reply_markup=get_persistent_keyboard())

# âœ… Ø§Ø³ØªØ§Ø±Øª
async def start(update: Update, context: ContextTypes.DEFAULT_TYPE):
    user_id = update.effective_user.id
    mode = load_user_mode(user_id)
    context.chat_data["mode"] = mode
    await update.message.reply_text("Ø³Ù„Ø§Ù…! Ù…Ù† Ø¢Ù…Ø§Ø¯Ù‡â€ŒØ§Ù… ğŸ§ ", reply_markup=get_persistent_keyboard())
    await show_mode_selection(update, context)

# ğŸ§  Ø§Ù†ØªØ®Ø§Ø¨ ÛŒØ§ Ø¨Ø§Ø²Ú¯Ø´Øª Ø¨Ù‡ Ø­Ø§Ù„Øª Ù‡ÙˆØ´ Ù…ØµÙ†ÙˆØ¹ÛŒ
async def handle_mode(update: Update, context: ContextTypes.DEFAULT_TYPE):
    query = update.callback_query
    await query.answer()
    user_id = query.from_user.id

    if query.data == "main_menu":
        await query.edit_message_text("Ù„Ø·ÙØ§Ù‹ ÛŒÚ© Ø­Ø§Ù„Øª Ù‡ÙˆØ´ Ù…ØµÙ†ÙˆØ¹ÛŒ Ø§Ù†ØªØ®Ø§Ø¨ Ú©Ù†ÛŒØ¯:")
        await show_mode_selection(update, context)
        return

    mode_map = {
        "set_gemini": "gemini",
        "set_openrouter": "openrouter",
        "set_deepseek": "deepseek",
        "set_refined": "refined"
    }
    selected_mode = mode_map.get(query.data, "gemini")
    context.chat_data["mode"] = selected_mode
    save_user_mode(user_id, selected_mode)
    await query.edit_message_text(f"âœ… Ù…Ø¯Ù„ Ø§Ù†ØªØ®Ø§Ø¨ Ø´Ø¯: {selected_mode.upper()}")

# ğŸ› ï¸ ØªØ§Ø¨Ø¹ ØªÙ‚Ø³ÛŒÙ… Ù…ØªÙ† Ø¨Ø±Ø§ÛŒ ØªÙ„Ú¯Ø±Ø§Ù…
def split_text_for_telegram(text, max_length=4000):
    parts = []
    current_part = ""
    
    sentences = re.split(r'(?<=[.!ØŸ])\s+', text.strip()) if text else [""]
    
    for sentence in sentences:
        if len(current_part) + len(sentence) + 1 > max_length:
            if current_part:
                parts.append(current_part.strip())
                current_part = sentence
            else:
                while len(sentence) > max_length:
                    split_index = sentence.rfind(" ", 0, max_length - 1)
                    if split_index == -1:
                        split_index = max_length
                    parts.append(sentence[:split_index].strip())
                    sentence = sentence[split_index:].strip()
                current_part = sentence
        else:
            current_part += (" " + sentence if current_part else sentence)
    
    if current_part:
        parts.append(current_part.strip())
    
    return parts if parts else [text]

# ğŸ’¬ Ù…Ø¯ÛŒØ±ÛŒØª Ù¾ÛŒØ§Ù… Ù…ØªÙ†ÛŒ Ú©Ø§Ø±Ø¨Ø±
async def handle_user_message(update: Update, context: ContextTypes.DEFAULT_TYPE):
    user_input = update.message.text
    mode = context.chat_data.get("mode", "gemini")

    print(f"\nğŸ“¥ Ù¾ÛŒØ§Ù… Ú©Ø§Ø±Ø¨Ø±: {user_input}")
    print(f"ğŸ›ï¸ Ø­Ø§Ù„Øª Ø§Ù†ØªØ®Ø§Ø¨â€ŒØ´Ø¯Ù‡: {mode}")

    loading_texts = [
        "â³ Ø¯Ø± Ø­Ø§Ù„ Ù¾Ø±Ø¯Ø§Ø²Ø´.",
        "â³ Ø¯Ø± Ø­Ø§Ù„ Ù¾Ø±Ø¯Ø§Ø²Ø´..",
        "â³ Ø¯Ø± Ø­Ø§Ù„ Ù¾Ø±Ø¯Ø§Ø²Ø´...",
        "â³ Ø¯Ø± Ø­Ø§Ù„ Ù¾Ø±Ø¯Ø§Ø²Ø´.....",
        "â³ Ø¯Ø± Ø­Ø§Ù„ Ù¾Ø±Ø¯Ø§Ø²Ø´..",
        "â³ Ø¯Ø± Ø­Ø§Ù„ Ù¾Ø±Ø¯Ø§Ø²Ø´.......",
        "â³ Ø¯Ø± Ø­Ø§Ù„ Ù¾Ø±Ø¯Ø§Ø²Ø´. . ."
    ]
    loading_message = await update.message.reply_text(random.choice(loading_texts))

    async def animate_loading():
        while not context.chat_data.get("done_processing", False):
            await asyncio.sleep(0.8)
            try:
                await loading_message.edit_text(random.choice(loading_texts))
            except:
                pass

    context.chat_data["done_processing"] = False
    loading_task = asyncio.create_task(animate_loading())

    try:
        reply = await process_message(user_input, mode=mode)
        context.chat_data["done_processing"] = True
        await loading_task

        if reply and reply.strip():
            await loading_message.delete()
            message_parts = split_text_for_telegram(reply, max_length=4000)
            for i, part in enumerate(message_parts):
                reply_markup = get_main_menu() if i == len(message_parts) - 1 else None
                await update.message.reply_text(part, reply_markup=reply_markup)
        else:
            await loading_message.edit_text("âŒ Ù¾Ø§Ø³Ø® Ø®Ø§Ù„ÛŒ Ø¨ÙˆØ¯.", reply_markup=get_main_menu())

    except Exception as e:
        context.chat_data["done_processing"] = True
        await loading_task
        error_msg = f"âŒ Ø®Ø·Ø§ Ù‡Ù†Ú¯Ø§Ù… Ø§Ø±Ø³Ø§Ù„ Ù¾Ø§Ø³Ø®:\n{str(e)}"
        print(error_msg)
        try:
            await loading_message.edit_text(error_msg, reply_markup=get_main_menu())
        except:
            await update.message.reply_text(error_msg, reply_markup=get_main_menu())

# ğŸ“· Ù…Ø¯ÛŒØ±ÛŒØª Ù¾ÛŒØ§Ù…â€ŒÙ‡Ø§ÛŒ Ø­Ø§ÙˆÛŒ Ø¹Ú©Ø³
async def handle_user_photo(update: Update, context: ContextTypes.DEFAULT_TYPE):
    user_id = update.effective_user.id
    mode = context.chat_data.get("mode", "gemini")
    
    photo = update.message.photo[-1]
    file = await context.bot.get_file(photo.file_id)
    
    loading_texts = [
        "â³ Ø¯Ø± Ø­Ø§Ù„ Ù¾Ø±Ø¯Ø§Ø²Ø´ ØªØµÙˆÛŒØ±...",
        "â³ Ù„Ø·ÙØ§Ù‹ ØµØ¨Ø± Ú©Ù†ÛŒØ¯...",
        "â³ Ù¾Ø±Ø¯Ø§Ø²Ø´ Ø¯Ø± Ø¬Ø±ÛŒØ§Ù† Ø§Ø³Øª..."
    ]
    loading_message = await update.message.reply_text(random.choice(loading_texts))

    photo_path = f"temp_{user_id}_{photo.file_id}.jpg"
    try:
        await file.download_to_drive(photo_path)
        
        response = await process_image(photo_path, update.message.caption or "ØªØµÙˆÛŒØ± Ø±Ø§ ØªÙˆØµÛŒÙ Ú©Ù†", mode)
        
        await loading_message.delete()
        
        message_parts = split_text_for_telegram(response, max_length=4000)
        for i, part in enumerate(message_parts):
            reply_markup = get_main_menu() if i == len(message_parts) - 1 else None
            await update.message.reply_text(part, reply_markup=reply_markup)
            
    except Exception as e:
        await loading_message.edit_text(f"âŒ Ø®Ø·Ø§ Ø¯Ø± Ù¾Ø±Ø¯Ø§Ø²Ø´ ØªØµÙˆÛŒØ±: {str(e)}", reply_markup=get_main_menu())
    finally:
        if os.path.exists(photo_path):
            os.remove(photo_path)

# ğŸš€ Ø§Ø¬Ø±Ø§ÛŒ Ø¨Ø§Øª
async def main():
    app = ApplicationBuilder().token(TELEGRAM_TOKEN).build()
    
    app.add_handler(CommandHandler("start", start))
    app.add_handler(CommandHandler("menu", show_main_menu))
    app.add_handler(CommandHandler("reset", reset_session))
    
    app.add_handler(CallbackQueryHandler(handle_mode))
    
    app.add_handler(MessageHandler(filters.TEXT & ~filters.COMMAND, handle_user_message))
    app.add_handler(MessageHandler(filters.PHOTO, handle_user_photo))
    
    print("ğŸ¤– Ø±Ø¨Ø§Øª Ø´Ø±ÙˆØ¹ Ø´Ø¯")
    await app.run_polling()

if __name__ == "__main__":
    import nest_asyncio
    nest_asyncio.apply()
    asyncio.run(main())